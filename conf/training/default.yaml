# @package training
defaults:
  - optim/lr_scheduler: exponential
  - optim/bn_scheduler: step_decay
  - optim/optimizer: Adam

# Those arguments defines the training hyper-parameters
epochs: 100
num_workers: 6
batch_size: 16
shuffle: True
cuda: 0 # -1 -> no cuda otherwise takes the specified index
precompute_multi_scale: False # Compute multiscate features on cpu for faster training / inference

optim:
  #optimizer params
  base_lr: 0.001
  grad_clip: -1

  # lr scheduler params
  momentum: 0
  dampening: 0
  weight_decay: 0

  # bn scheduler params
  bn_momentum: 0.1
  bn_decay: 0.9
  bn_decay_step: 10
  bn_clip: 1e-2

weight_name: "latest" # Used during resume, select with model to load from [miou, macc, acc..., latest]
enable_cudnn: True
checkpoint_dir: ""

# Those arguments within experiment defines which model, dataset and task to be created for benchmarking
# parameters for Weights and Biases
wandb:
  entity: ""
  project: default
  log: True
  notes:
  name:
  public: True # It will be display the model within wandb log, else not.
  config:
    model_name: ${model_name}

# parameters for TensorBoard Visualization
tensorboard:
  log: False
